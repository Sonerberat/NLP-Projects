{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67dc56b0",
   "metadata": {},
   "source": [
    "## Installing some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ce4b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: torch in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: filelock in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (10.0.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (2022.10.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (1.5.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: dill<0.3.6 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas->datasets) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\soner\\anaconda3\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd139d",
   "metadata": {},
   "source": [
    "## Download dataset from bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db240a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "url = 'https://storage.googleapis.com/nlprefresh-public/tydiqa_data.zip'\n",
    "filename=wget.download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0746430",
   "metadata": {},
   "source": [
    "## Unzip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4195e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "path=r\"C:\\Users\\soner\\OneDrive\\Desktop\\NLP project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85bbf36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Chatbot',\n",
       " 'Ouestion Answering.ipynb',\n",
       " 'Question Answersing',\n",
       " 'tydiqa_data.zip']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbcfee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('tydiqa_data.zip') as f:\n",
    "    f.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109edbdd",
   "metadata": {},
   "source": [
    "## To access the preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c75d0566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soner\\anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['passage_answer_candidates', 'question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url'],\n",
       "        num_rows: 9211\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['passage_answer_candidates', 'question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url'],\n",
       "        num_rows: 1031\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "#The path where the dataset is stored\n",
    "path = 'tydiqa_data'\n",
    "\n",
    "#Load Dataset\n",
    "tydiqa_data = load_from_disk(path)\n",
    "\n",
    "tydiqa_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d8527",
   "metadata": {},
   "source": [
    "## Checking the object type for one of the elements in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ff3a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tydiqa_data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce4ee1",
   "metadata": {},
   "source": [
    "## the structure of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c7aec69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['passage_answer_candidates', 'question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url'],\n",
       "    num_rows: 9211\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tydiqa_data['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909239d2",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b5e901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What mental effects can a mother experience after childbirth?\n",
      "\n",
      "Context (truncated): \n",
      "\n",
      "Postpartum depression (PPD), also called postnatal depression, is a type of mood disorder associated with childbirth, which can affect both sexes.[1][3] Symptoms may include extreme sadness, low energy, anxiety, crying episodes, irritability, and changes in sleeping or eating patterns.[1] Onset is typically between one week and one month following childbirth.[1] PPD can also negatively affect the newborn child.[2]\n",
      "\n",
      "While the exact cause of PPD is unclear, the cause is believed to be a combination of physi...\n",
      "\n",
      "Answer: Postpartum depression (PPD)\n"
     ]
    }
   ],
   "source": [
    "idx = 600\n",
    "\n",
    "# start index\n",
    "start_index = tydiqa_data['train'][idx]['annotations']['minimal_answers_start_byte'][0]\n",
    "\n",
    "# end index\n",
    "end_index = tydiqa_data['train'][idx]['annotations']['minimal_answers_end_byte'][0]\n",
    "\n",
    "print(\"Question: \" + tydiqa_data['train'][idx]['question_text'])\n",
    "print(\"\\nContext (truncated): \"+ tydiqa_data['train'][idx]['document_plaintext'][0:512] + '...')\n",
    "print(\"\\nAnswer: \" + tydiqa_data['train'][idx]['document_plaintext'][start_index:end_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63815554",
   "metadata": {},
   "source": [
    "### The dataset contains unanswerable questions. For these, the start and end indices for the answer are equal to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bc7ab3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passage_answer_candidate_index': [-1],\n",
       " 'minimal_answers_start_byte': [-1],\n",
       " 'minimal_answers_end_byte': [-1],\n",
       " 'yes_no_answer': ['NONE']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tydiqa_data['train'][0]['annotations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a623cf8",
   "metadata": {},
   "source": [
    "## flatten the dataset to work with an object with a table structure instead of a dictionary structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83e495f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_train_data = tydiqa_data['train'].flatten()\n",
    "flattened_test_data =  tydiqa_data['validation'].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb248a50",
   "metadata": {},
   "source": [
    "## Selecting a subset of the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f477316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a subset of the train dataset\n",
    "flattened_train_data = flattened_train_data.select(range(3000))\n",
    "\n",
    "# Selecting a subset of the test dataset\n",
    "flattened_test_data = flattened_test_data.select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833439d2",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e746d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7836c46f",
   "metadata": {},
   "source": [
    "##  Processing samples using the 3 steps described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cee14cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_samples(sample):    \n",
    "    tokenized_data = tokenizer(sample['document_plaintext'], sample['question_text'], truncation=\"only_first\", padding=\"max_length\")\n",
    "    \n",
    "    input_ids = tokenized_data[\"input_ids\"]\n",
    "        \n",
    "    # We will label impossible answers with the index of the CLS token.\n",
    "    cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        \n",
    "    # If no answers are given, set the cls_index as answer.\n",
    "    if sample[\"annotations.minimal_answers_start_byte\"][0] == -1:\n",
    "        start_position = cls_index\n",
    "        end_position = cls_index\n",
    "    else:\n",
    "        # Start/end character index of the answer in the text.\n",
    "        gold_text = sample[\"document_plaintext\"][sample['annotations.minimal_answers_start_byte'][0]:sample['annotations.minimal_answers_end_byte'][0]]\n",
    "        start_char = sample[\"annotations.minimal_answers_start_byte\"][0]\n",
    "        end_char = sample['annotations.minimal_answers_end_byte'][0] #start_char + len(gold_text)\n",
    "\n",
    "        # sometimes answers are off by a character or two – fix this\n",
    "        if sample['document_plaintext'][start_char-1:end_char-1] == gold_text:\n",
    "            start_char = start_char - 1\n",
    "            end_char = end_char - 1     # When the gold label is off by one character\n",
    "        elif sample['document_plaintext'][start_char-2:end_char-2] == gold_text:\n",
    "            start_char = start_char - 2\n",
    "            end_char = end_char - 2     # When the gold label is off by two characters\n",
    "                                  \n",
    "        start_token = tokenized_data.char_to_token(start_char)\n",
    "        end_token = tokenized_data.char_to_token(end_char - 1)\n",
    "        \n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_token is None:\n",
    "            start_token = tokenizer.model_max_length\n",
    "        if end_token is None:\n",
    "            end_token = tokenizer.model_max_length\n",
    "            \n",
    "        start_position = start_token\n",
    "        end_position = end_token\n",
    "\n",
    "    return {'input_ids': tokenized_data['input_ids'],\n",
    "          'attention_mask': tokenized_data['attention_mask'],\n",
    "          'start_positions': start_position,\n",
    "          'end_positions': end_position}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce15b7b4",
   "metadata": {},
   "source": [
    "## Tokenizing and processing the flattened dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e209c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:40<00:00, 74.18ex/s] \n",
      "100%|██████████| 1000/1000 [00:15<00:00, 64.52ex/s]\n"
     ]
    }
   ],
   "source": [
    "processed_train_data = flattened_train_data.map(process_samples)\n",
    "processed_test_data = flattened_test_data.map(process_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a76dc45",
   "metadata": {},
   "source": [
    " # Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58c1987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d2a57b",
   "metadata": {},
   "source": [
    "## taking the necessary columns from the datasets to train/test and return them as Pytorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1d0aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_return = ['input_ids','attention_mask', 'start_positions', 'end_positions']\n",
    "processed_train_data.set_format(type='pt', columns=columns_to_return) \n",
    "processed_test_data.set_format(type='pt', columns=columns_to_return) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f51fd",
   "metadata": {},
   "source": [
    "## the F1 score as a metric to evaluate your model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c8b7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_f1_metrics(pred):    \n",
    "    start_labels = pred.label_ids[0]\n",
    "    start_preds = pred.predictions[0].argmax(-1)\n",
    "    end_labels = pred.label_ids[1]\n",
    "    end_preds = pred.predictions[1].argmax(-1)\n",
    "    \n",
    "    f1_start = f1_score(start_labels, start_preds, average='macro')\n",
    "    f1_end = f1_score(end_labels, end_preds, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'f1_start': f1_start,\n",
    "        'f1_end': f1_end,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59c4cd9",
   "metadata": {},
   "source": [
    "## The Trainer to fine-tune your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fc58dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: document_plaintext, passage_answer_candidates.plaintext_start_byte, annotations.minimal_answers_start_byte, annotations.yes_no_answer, annotations.passage_answer_candidate_index, passage_answer_candidates.plaintext_end_byte, annotations.minimal_answers_end_byte, language, document_title, document_url, question_text. If document_plaintext, passage_answer_candidates.plaintext_start_byte, annotations.minimal_answers_start_byte, annotations.yes_no_answer, annotations.passage_answer_candidate_index, passage_answer_candidates.plaintext_end_byte, annotations.minimal_answers_end_byte, language, document_title, document_url, question_text are not expected by `DistilBertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\soner\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1125\n",
      "  Number of trainable parameters = 64799234\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 07:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.643300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.802500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.327600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.686600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.672200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.805400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.618600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.664400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model_results5\\checkpoint-500\n",
      "Configuration saved in model_results5\\checkpoint-500\\config.json\n",
      "Model weights saved in model_results5\\checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to model_results5\\checkpoint-1000\n",
      "Configuration saved in model_results5\\checkpoint-1000\\config.json\n",
      "Model weights saved in model_results5\\checkpoint-1000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1125, training_loss=1.2328780848185221, metrics={'train_runtime': 437.7623, 'train_samples_per_second': 20.559, 'train_steps_per_second': 2.57, 'total_flos': 1175877900288000.0, 'train_loss': 1.2328780848185221, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='model_results5',          # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=20,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=None,            # directory for storing logs\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # the instantiated Transformers model to be trained\n",
    "    args=training_args, # training arguments, defined above\n",
    "    train_dataset=processed_train_data, # training dataset\n",
    "    eval_dataset=processed_test_data, # evaluation dataset\n",
    "    compute_metrics=compute_f1_metrics             \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435210e9",
   "metadata": {},
   "source": [
    "## Evaluating the fine-tuned model's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b8f425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: document_plaintext, passage_answer_candidates.plaintext_start_byte, annotations.minimal_answers_start_byte, annotations.yes_no_answer, annotations.passage_answer_candidate_index, passage_answer_candidates.plaintext_end_byte, annotations.minimal_answers_end_byte, language, document_title, document_url, question_text. If document_plaintext, passage_answer_candidates.plaintext_start_byte, annotations.minimal_answers_start_byte, annotations.yes_no_answer, annotations.passage_answer_candidate_index, passage_answer_candidates.plaintext_end_byte, annotations.minimal_answers_end_byte, language, document_title, document_url, question_text are not expected by `DistilBertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.2335121631622314,\n",
       " 'eval_f1_start': 0.10161689129800391,\n",
       " 'eval_f1_end': 0.10969190042436358,\n",
       " 'eval_runtime': 16.071,\n",
       " 'eval_samples_per_second': 62.224,\n",
       " 'eval_steps_per_second': 7.778,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(processed_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e413d",
   "metadata": {},
   "source": [
    "## Using Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7db19fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What superheroes were introduced between 1939 and 1941 by Detective Comics and its sister company?\n",
      "Answer: Batman and Robin, Wonder Woman, the Flash, Green Lantern, Doctor Fate, the Atom, Hawkman, Green Arrow and Aquaman\n",
      "\n",
      "Question: What comic book characters were created between 1939 and 1941?\n",
      "Answer: Superman, Batman, Captain Marvel ( later known as SHAZAM! ), Captain America, and Wonder Woman\n",
      "\n",
      "Question: What well-known characters were created between 1939 and 1941?\n",
      "Answer: Superman, Batman, Captain Marvel ( later known as SHAZAM! ), Captain America, and Wonder Woman\n",
      "\n",
      "Question: What well-known superheroes were introduced between 1939 and 1941 by Detective Comics?\n",
      "Answer: Superman, Batman, Captain Marvel ( later known as SHAZAM! ), Captain America, and Wonder Woman. Between 1939 and 1941 Detective Comics and its sister company, All - American Publications, introduced popular superheroes such as Batman and Robin, Wonder Woman, the Flash, Green Lantern, Doctor Fate, the Atom, Hawkman, Green Arrow and Aquaman\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "text = r\"\"\"\n",
    "The Golden Age of Comic Books describes an era of American comic books from the \n",
    "late 1930s to circa 1950. During this time, modern comic books were first published \n",
    "and rapidly increased in popularity. The superhero archetype was created and many \n",
    "well-known characters were introduced, including Superman, Batman, Captain Marvel \n",
    "(later known as SHAZAM!), Captain America, and Wonder Woman.\n",
    "Between 1939 and 1941 Detective Comics and its sister company, All-American Publications, \n",
    "introduced popular superheroes such as Batman and Robin, Wonder Woman, the Flash, \n",
    "Green Lantern, Doctor Fate, the Atom, Hawkman, Green Arrow and Aquaman.[7] Timely Comics, \n",
    "the 1940s predecessor of Marvel Comics, had million-selling titles featuring the Human Torch,\n",
    "the Sub-Mariner, and Captain America.[8]\n",
    "As comic books grew in popularity, publishers began launching titles that expanded \n",
    "into a variety of genres. Dell Comics' non-superhero characters (particularly the \n",
    "licensed Walt Disney animated-character comics) outsold the superhero comics of the day.[12] \n",
    "The publisher featured licensed movie and literary characters such as Mickey Mouse, Donald Duck,\n",
    "Roy Rogers and Tarzan.[13] It was during this era that noted Donald Duck writer-artist\n",
    "Carl Barks rose to prominence.[14] Additionally, MLJ's introduction of Archie Andrews\n",
    "in Pep Comics #22 (December 1941) gave rise to teen humor comics,[15] with the Archie \n",
    "Andrews character remaining in print well into the 21st century.[16]\n",
    "At the same time in Canada, American comic books were prohibited importation under \n",
    "the War Exchange Conservation Act[17] which restricted the importation of non-essential \n",
    "goods. As a result, a domestic publishing industry flourished during the duration \n",
    "of the war which were collectively informally called the Canadian Whites.\n",
    "The educational comic book Dagwood Splits the Atom used characters from the comic \n",
    "strip Blondie.[18] According to historian Michael A. Amundson, appealing comic-book \n",
    "characters helped ease young readers' fear of nuclear war and neutralize anxiety \n",
    "about the questions posed by atomic power.[19] It was during this period that long-running \n",
    "humor comics debuted, including EC's Mad and Carl Barks' Uncle Scrooge in Dell's Four \n",
    "Color Comics (both in 1952).[20][21]\n",
    "\"\"\"\n",
    "\n",
    "questions = [\"What superheroes were introduced between 1939 and 1941 by Detective Comics and its sister company?\",\n",
    "             \"What comic book characters were created between 1939 and 1941?\",\n",
    "             \"What well-known characters were created between 1939 and 1941?\",\n",
    "             \"What well-known superheroes were introduced between 1939 and 1941 by Detective Comics?\"]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\n",
    "    #print(\"inputs\", inputs)\n",
    "    #print(\"inputs\", type(inputs))\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    inputs.to(\"cuda\")\n",
    "\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer_model = model(**inputs)\n",
    "\n",
    "    answer_start = torch.argmax(\n",
    "        answer_model['start_logits']\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_model['end_logits']) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
